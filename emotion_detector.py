#!/usr/bin/env python3

import cv2
import numpy as np
import time
import os
import sys
from datetime import datetime
import subprocess
import threading
import pandas as pd
import openpyxl
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment
from collections import deque
import queue

# ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
FRAME_SKIP = 1  # ‡πÑ‡∏°‡πà‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏ü‡∏£‡∏° (real-time detection)
EMOTION_CACHE_SIZE = 3  # ‡∏•‡∏î‡πÅ‡∏Ñ‡∏ä‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
EXCEL_SAVE_INTERVAL = 5  # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Excel ‡∏ó‡∏∏‡∏Å 5 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
MAX_QUEUE_SIZE = 100  # ‡∏Ç‡∏ô‡∏≤‡∏î‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏¥‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
DETECTION_INTERVAL = 0.05  # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ó‡∏∏‡∏Å 50ms (20 FPS)
DATA_SAVE_INTERVAL = 5.0  # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å 5 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ

try:
    from picamera2 import Picamera2
    from libcamera import controls
    PICAMERA2_AVAILABLE = True
    print("‚úÖ PiCamera2 library loaded successfully")
except ImportError:
    PICAMERA2_AVAILABLE = False
    print("‚ö†Ô∏è PiCamera2 not available, falling back to OpenCV")

try:
    from deepface import DeepFace
    DEEPFACE_AVAILABLE = True
    print("‚úÖ DeepFace library loaded successfully")
except ImportError:
    DEEPFACE_AVAILABLE = False
    print("‚ö†Ô∏è DeepFace not installed. Using simple face detection only.")
    print("Install with: pip install deepface tensorflow")

class RaspberryPi4CameraDetector:
    def __init__(self):
        self.cap = None
        self.picam2 = None
        self.camera_method = None
        self.face_cascade = None
        self.emotion_history = deque(maxlen=100)  # ‡πÉ‡∏ä‡πâ deque ‡πÅ‡∏ó‡∏ô list
        self.is_running = False
        self.frame_buffer = None
        self.buffer_lock = threading.Lock()
        self.color_mode = "color"
        self.auto_exposure = True
        self.brightness = 0.0
        self.contrast = 1.0
        self.camera_type = None
        self.excel_file = "emotion_data.xlsx"
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
        self.frame_count = 0
        self.last_emotion_time = 0
        self.last_data_save_time = time.time()  # ‡πÄ‡∏ß‡∏•‡∏≤‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        self.emotion_cache = {}  # ‡πÅ‡∏Ñ‡∏ä‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå
        self.data_queue = queue.Queue(maxsize=MAX_QUEUE_SIZE)
        self.excel_save_counter = 0
        self.current_emotion_data = None  # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö manual emotion input
        self.manual_emotion = None
        self.manual_confidence = 0.0
        self.manual_mode = False
        self.last_manual_time = 0
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Excel ‡πÅ‡∏ö‡∏ö time-based
        self.last_excel_save_time = time.time()
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≠
        self.fullscreen_mode = True
        
        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ò‡∏£‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        self.save_thread = threading.Thread(target=self._save_data_worker, daemon=True)
        self.save_thread.start()
        
        self.load_face_cascade()
        self.initialize_excel()

    def _save_data_worker(self):
        """‡πÄ‡∏ò‡∏£‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á Excel"""
        while self.is_running:
            try:
                data = self.data_queue.get(timeout=1)
                if data:
                    self._save_to_excel_internal(*data)
                    self.excel_save_counter += 1
                    
                    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏∏‡∏Å‡πÜ 5 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
                    current_time = time.time()
                    if current_time - self.last_excel_save_time >= EXCEL_SAVE_INTERVAL:
                        self._save_excel_file()
                        self.last_excel_save_time = current_time
                        print(f"üíæ Excel saved - {self.excel_save_counter} records")
            except queue.Empty:
                continue
            except Exception as e:
                print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}")

    def _save_excel_file(self):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel"""
        try:
            if hasattr(self, '_wb'):
                self._wb.save(self.excel_file)
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel: {e}")

    def initialize_excel(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Excel ‡πÉ‡∏´‡∏°‡πà‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ"""
        try:
            print(f"üîç Checking Excel file: {self.excel_file}")
            
            if not os.path.exists(self.excel_file):
                print("üìù Creating new Excel file...")
                self._wb = Workbook()
                ws = self._wb.active
                ws.title = "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå"
                
                headers = [
                    "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", "‡πÄ‡∏ß‡∏•‡∏≤", "‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå", "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à (%)", 
                    "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à", "‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô", "‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏•‡πâ‡∏≠‡∏á"
                ]
                
                header_font = Font(bold=True, size=12)
                header_fill = PatternFill(start_color="CCE5FF", end_color="CCE5FF", fill_type="solid")
                
                for col, header in enumerate(headers, 1):
                    cell = ws.cell(row=1, column=col)
                    cell.value = header
                    cell.font = header_font
                    cell.fill = header_fill
                    cell.alignment = Alignment(horizontal='center')
                
                for col in range(1, len(headers) + 1):
                    ws.column_dimensions[chr(64 + col)].width = 15
                
                self._wb.save(self.excel_file)
                print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Excel ‡πÉ‡∏´‡∏°‡πà: {self.excel_file}")
                print(f"üìÅ File path: {os.path.abspath(self.excel_file)}")
            else:
                print(f"‚úÖ Excel file already exists: {self.excel_file}")
                self._wb = openpyxl.load_workbook(self.excel_file)
                
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Excel: {e}")
            import traceback
            traceback.print_exc()

    def save_to_excel(self, emotion, confidence, satisfaction_level, satisfaction_text):
        """‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÉ‡∏ô‡∏Ñ‡∏¥‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å (‡∏ó‡∏∏‡∏Å 5 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)"""
        try:
            current_time = time.time()
            
            # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÑ‡∏ß‡πâ
            self.current_emotion_data = {
                'emotion': emotion,
                'confidence': confidence,
                'satisfaction_level': satisfaction_level,
                'satisfaction_text': satisfaction_text,
                'timestamp': current_time
            }
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å 5 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
            if current_time - self.last_data_save_time >= DATA_SAVE_INTERVAL:
                now = datetime.now()
                data = (
                    now.strftime("%Y-%m-%d"),
                    now.strftime("%H:%M:%S"),
                    emotion,
                    f"{confidence:.2f}",
                    satisfaction_level,
                    satisfaction_text,
                    self.camera_method
                )
                
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á Excel ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                self._save_to_excel_internal(*data)
                self._save_excel_file()
                
                self.last_data_save_time = current_time
                print(f"üíæ Data saved to Excel: {emotion} ({confidence:.1f}%)")
                
        except Exception as e:
            print(f"‚ùå Error saving to Excel: {e}")

    def _save_to_excel_internal(self, date, time, emotion, confidence, satisfaction_level, satisfaction_text, camera_method):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel"""
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå Excel ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if not os.path.exists(self.excel_file):
                print("üìù Creating new Excel file...")
                self.initialize_excel()
            
            # ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel
            if not hasattr(self, '_wb') or self._wb is None:
                self._wb = openpyxl.load_workbook(self.excel_file)
            
            ws = self._wb.active
            
            # ‡∏´‡∏≤‡πÅ‡∏ñ‡∏ß‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
            next_row = ws.max_row + 1
            data = [date, time, emotion, confidence, satisfaction_level, satisfaction_text, camera_method]
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            for col, value in enumerate(data, 1):
                cell = ws.cell(row=next_row, column=col)
                cell.value = value
                cell.alignment = Alignment(horizontal='center')
            
            print(f"‚úÖ Data added to Excel row {next_row}: {emotion} ({confidence}%)")
            
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á Excel: {e}")
            import traceback
            traceback.print_exc()

    def load_face_cascade(self):
        """‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ cv2.data ‡πÉ‡∏ô Raspberry Pi 4 ‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î cascade ‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö"""
        try:
            # ‡∏•‡∏≠‡∏á‡∏´‡∏≤ cascade files ‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ï‡πà‡∏≤‡∏á‡πÜ   
            possible_paths = [
                '/usr/share/opencv4/haarcascades/haarcascade_frontalface_default.xml',
                '/usr/local/share/opencv4/haarcascades/haarcascade_frontalface_default.xml',
                '/home/pi/.local/lib/python3.*/site-packages/cv2/data/haarcascade_frontalface_default.xml'
            ]
            
            # ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ cv2.data ‡∏Å‡πà‡∏≠‡∏ô
            try:
                cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
                if os.path.exists(cascade_path):
                    self.face_cascade = cv2.CascadeClassifier(cascade_path)
                    print("‚úÖ Face cascade loaded from cv2.data")
                    return
            except AttributeError:
                pass
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏≠‡∏∑‡πà‡∏ô
            for path in possible_paths:
                if os.path.exists(path):
                    self.face_cascade = cv2.CascadeClassifier(path)
                    print(f"‚úÖ Face cascade loaded from {path}")
                    return
            
            # ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠
            print("üì• Downloading Haar cascade file...")
            import urllib.request
            url = "https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml"
            cascade_path = "haarcascade_frontalface_default.xml"
            urllib.request.urlretrieve(url, cascade_path)
            self.face_cascade = cv2.CascadeClassifier(cascade_path)
            print("‚úÖ Face cascade downloaded and loaded")
            
        except Exception as e:
            print(f"‚ùå Error loading face cascade: {e}")
            self.face_cascade = None
    
    def enhance_frame_for_detection(self, frame):
        """‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏ü‡∏£‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô"""
        try:
            # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≠‡∏ô‡∏ó‡∏£‡∏≤‡∏™‡∏ï‡πå
            enhanced = cv2.convertScaleAbs(frame, alpha=1.2, beta=10)
            
            # ‡∏•‡∏î noise
            enhanced = cv2.bilateralFilter(enhanced, 9, 75, 75)
            
            # ‡∏õ‡∏£‡∏±‡∏ö histogram
            if len(enhanced.shape) == 3:
                # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô LAB color space
                lab = cv2.cvtColor(enhanced, cv2.COLOR_BGR2LAB)
                l, a, b = cv2.split(lab)
                
                # ‡∏õ‡∏£‡∏±‡∏ö L channel ‡∏î‡πâ‡∏ß‡∏¢ CLAHE
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
                l = clahe.apply(l)
                
                # ‡∏£‡∏ß‡∏°‡∏Å‡∏•‡∏±‡∏ö
                enhanced = cv2.merge([l, a, b])
                enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)
            
            return enhanced
        except Exception as e:
            print(f"Frame enhancement error: {e}")
            return frame
    
    def check_camera_hardware(self):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Æ‡∏≤‡∏£‡πå‡∏î‡πÅ‡∏ß‡∏£‡πå‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÉ‡∏ô Raspberry Pi 4"""
        print("üîç Enhanced camera status check...")
        print("=" * 50)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö libcamera
        try:
            result = subprocess.run(['libcamera-hello', '--list-cameras'], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                print("‚úÖ libcamera tools available")
                print("üì∑ Available cameras:")
                print(result.stdout)
            else:
                print("‚ö†Ô∏è libcamera available but no cameras detected")
        except (subprocess.TimeoutExpired, FileNotFoundError):
            print("‚ùå libcamera tools not found")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö video devices
        print("\nüìπ Video devices:")
        os.system("ls -la /dev/video* 2>/dev/null || echo '‚ö†Ô∏è No video devices found'")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö vcgencmd
        print("\n‚öôÔ∏è Camera configuration:")
        os.system("vcgencmd get_camera 2>/dev/null || echo '‚ö†Ô∏è Cannot get camera status'")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö config.txt
        print("\nüìù Config.txt camera settings:")
        os.system("grep -E '(camera|start_x|gpu_mem)' /boot/config.txt 2>/dev/null || echo '‚ö†Ô∏è No camera settings found'")
        
        print("=" * 50)
    
    def setup_picamera2(self):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ PiCamera2 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Raspberry Pi 4 ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏µ"""
        try:
            if not PICAMERA2_AVAILABLE:
                return False
            
            print("üîÑ Trying PiCamera2 method...")
            
            self.picam2 = Picamera2()
            
            # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ preview config ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            if self.color_mode == "grayscale":
                config = self.picam2.create_preview_configuration(
                    main={"size": (640, 480), "format": "YUV420"},
                    controls={"FrameRate": 30}
                )
            else:
                config = self.picam2.create_preview_configuration(
                    main={"size": (640, 480), "format": "RGB888"},
                    controls={"FrameRate": 30}
                )
            
            self.picam2.configure(config)
            
            # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ controls ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏µ
            control_settings = {
                "AeEnable": self.auto_exposure,
                "AwbEnable": True,  # Auto White Balance
                "AwbMode": controls.AwbModeEnum.Auto,
                "Brightness": self.brightness,
                "Contrast": self.contrast,
                "Saturation": 1.0,
                "Sharpness": 1.0
            }
            
            self.picam2.set_controls(control_settings)
            
            self.picam2.start()
            time.sleep(2)  # ‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏á
            
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏ö‡∏†‡∏≤‡∏û
            frame = self.picam2.capture_array()
            if frame is not None and frame.size > 0:
                print(f"‚úÖ PiCamera2 method successful - Format: {frame.shape}")
                print(f"üì∑ Color mode: {self.color_mode}")
                self.camera_method = "picamera2"
                self.start_frame_capture_thread()
                return True
            else:
                self.picam2.stop()
                return False
                
        except Exception as e:
            print(f"   PiCamera2 error: {e}")
            if self.picam2:
                try:
                    self.picam2.stop()
                except:
                    pass
            return False
    
    def start_frame_capture_thread(self):
        """‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ò‡∏£‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á"""
        def capture_frames():
            while self.is_running:
                try:
                    if self.picam2:
                        frame = self.picam2.capture_array()
                        with self.buffer_lock:
                            self.frame_buffer = frame.copy()
                    time.sleep(0.03)  # ~30 FPS
                except Exception as e:
                    print(f"Frame capture error: {e}")
                    break
        
        self.is_running = True
        self.capture_thread = threading.Thread(target=capture_frames, daemon=True)
        self.capture_thread.start()
    
    def setup_opencv_camera(self):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢ OpenCV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Raspberry Pi 4 ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏µ"""
        try:
            print("üîÑ Trying simple OpenCV camera access...")
            
            # ‡∏•‡∏≠‡∏á‡∏Å‡∏•‡πâ‡∏≠‡∏á USB ‡πÅ‡∏ö‡∏ö‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤‡∏Å‡πà‡∏≠‡∏ô (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ V4L2 ‡∏´‡∏£‡∏∑‡∏≠ GStreamer)
            for i in range(5):  # ‡∏•‡∏≠‡∏á 5 devices
                print(f"   Testing camera index {i}...")
                self.cap = cv2.VideoCapture(i)
                
                if self.cap.isOpened():
                    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏•‡∏∞ FPS
                    self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
                    self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
                    self.cap.set(cv2.CAP_PROP_FPS, 30)
                    self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
                    
                    ret, frame = self.cap.read()
                    if ret and frame is not None:
                        print(f"‚úÖ USB camera found at index {i} - Shape: {frame.shape}")
                        self.camera_method = f"usb_{i}"
                        return True
                    self.cap.release()
                else:
                    print(f"   Camera {i} not available")
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏•‡∏≠‡∏á GStreamer pipeline
            print("üîÑ Trying GStreamer pipeline...")
            try:
                if self.color_mode == "grayscale":
                    gst_pipeline = (
                        "libcamerasrc ! "
                        "video/x-raw,width=640,height=480,framerate=30/1,format=GRAY8 ! "
                        "videoconvert ! appsink"
                    )
                else:
                    gst_pipeline = (
                        "libcamerasrc ! "
                        "video/x-raw,width=640,height=480,framerate=30/1 ! "
                        "videoconvert ! appsink"
                    )
                
                self.cap = cv2.VideoCapture(gst_pipeline, cv2.CAP_GSTREAMER)
                
                if self.cap.isOpened():
                    ret, frame = self.cap.read()
                    if ret and frame is not None:
                        print(f"‚úÖ GStreamer pipeline successful - Shape: {frame.shape}")
                        self.camera_method = "gstreamer"
                        return True
                    self.cap.release()
            except Exception as e:
                print(f"   GStreamer error: {e}")
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏•‡∏≠‡∏á V4L2
            print("üîÑ Trying V4L2 direct access...")
            try:
                # ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô bcm2835-v4l2 module
                os.system("sudo modprobe bcm2835-v4l2 2>/dev/null")
                time.sleep(1)
                
                self.cap = cv2.VideoCapture(0, cv2.CAP_V4L2)
                
                if self.cap.isOpened():
                    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏•‡∏∞ FPS
                    self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
                    self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
                    self.cap.set(cv2.CAP_PROP_FPS, 30)
                    self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
                    
                    ret, frame = self.cap.read()
                    if ret and frame is not None:
                        print(f"‚úÖ V4L2 method successful - Shape: {frame.shape}")
                        self.camera_method = "v4l2"
                        return True
                    self.cap.release()
            except Exception as e:
                print(f"   V4L2 error: {e}")
            
            print("‚ùå No working camera found with OpenCV")
            return False
            
        except Exception as e:
            print(f"   OpenCV camera error: {e}")
            if self.cap:
                self.cap.release()
            return False
    
    def setup_laptop_camera(self):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÇ‡∏ô‡πä‡∏ï‡∏ö‡∏∏‡πä‡∏Ñ"""
        try:
            print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÄ‡∏ß‡πá‡∏ö‡πÅ‡∏Ñ‡∏°...")
            self.cap = cv2.VideoCapture(0)  # ‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠
            
            if self.cap.isOpened():
                # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏•‡∏∞ FPS
                self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
                self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
                self.cap.set(cv2.CAP_PROP_FPS, 30)
                
                ret, frame = self.cap.read()
                if ret and frame is not None:
                    print(f"‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÄ‡∏ß‡πá‡∏ö‡πÅ‡∏Ñ‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à - ‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û: {frame.shape}")
                    self.camera_method = "laptop_webcam"
                    return True
                self.cap.release()
            
            return False
            
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÄ‡∏ß‡πá‡∏ö‡πÅ‡∏Ñ‡∏°: {e}")
            if self.cap:
                self.cap.release()
            return False
    
    def detect_usb_camera(self):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏•‡πâ‡∏≠‡∏á USB ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        try:
            print("üîç Checking for USB cameras...")
            
            # ‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÅ‡∏ö‡∏ö‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤‡∏Å‡πà‡∏≠‡∏ô
            for i in range(5):
                cap = cv2.VideoCapture(i)
                if cap.isOpened():
                    ret, frame = cap.read()
                    if ret and frame is not None:
                        print(f"‚úÖ Working camera found at index {i}")
                        cap.release()
                        return True
                    cap.release()
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö video devices
            import glob
            video_devices = glob.glob('/dev/video*')
            if video_devices:
                print(f"üìπ Found video devices: {video_devices}")
                
                # ‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß
                for device in video_devices:
                    try:
                        device_num = int(device.split('video')[1])
                        cap = cv2.VideoCapture(device_num)
                        if cap.isOpened():
                            ret, frame = cap.read()
                            if ret and frame is not None:
                                print(f"‚úÖ Working camera found at {device}")
                                cap.release()
                                return True
                            cap.release()
                    except:
                        continue
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö USB devices
            import subprocess
            try:
                result = subprocess.run(['lsusb'], capture_output=True, text=True, timeout=5)
                if 'camera' in result.stdout.lower() or 'webcam' in result.stdout.lower():
                    print("üì∑ USB camera detected in lsusb")
                    return True
            except:
                pass
            
            print("‚ö†Ô∏è No USB cameras detected")
            return False
            
        except Exception as e:
            print(f"‚ùå Error detecting USB camera: {e}")
            return False
    
    def set_manual_emotion(self, emotion, confidence):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡πÅ‡∏ö‡∏ö manual"""
        self.manual_emotion = emotion
        self.manual_confidence = confidence
        self.last_manual_time = time.time()
        print(f"üéØ Manual emotion set: {emotion} ({confidence:.1f}%)")
    
    def get_emotion_assessment(self, confidence):
        """‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÉ‡∏´‡∏°‡πà"""
        if confidence >= 80:
            return "Happy", "*****", 5
        elif confidence >= 50:
            return "Neutral", "***", 3
        elif confidence >= 30:
            return "Sad", "**", 2
        else:
            return "Angry", "*", 1
    
    def setup_camera(self):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å"""
        print("üîç Camera setup...")
        
        if self.camera_type == "laptop":
            return self.setup_laptop_camera()
        elif self.camera_type == "pi":
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏•‡πâ‡∏≠‡∏á USB ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á OpenCV ‡∏Å‡πà‡∏≠‡∏ô
            print("üîå Detecting camera type...")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏•‡πâ‡∏≠‡∏á USB ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            usb_camera_found = self.detect_usb_camera()
            
            if usb_camera_found:
                print("üì∑ USB camera detected, using OpenCV method")
                if self.setup_opencv_camera():
                    return True
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏•‡πâ‡∏≠‡∏á USB ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ‡∏•‡∏≠‡∏á PiCamera2 (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CSI camera)
            print("üì∑ Trying PiCamera2 for CSI camera...")
            if self.setup_picamera2():
                return True
            
            # ‡∏ñ‡πâ‡∏≤ PiCamera2 ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏•‡∏≠‡∏á OpenCV ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
            print("üîÑ Fallback to OpenCV...")
            if self.setup_opencv_camera():
                return True
        
        print("‚ùå No camera found. Troubleshooting steps:")
        print("   1. For USB cameras: Check USB connection and permissions")
        print("   2. For Pi Camera Module: Check CSI cable and enable camera")
        print("   3. Enable camera: sudo raspi-config")
        print("   4. Add to /boot/config.txt: camera_auto_detect=1")
        print("   5. Increase GPU memory: gpu_mem=128")
        print("   6. Reboot system")
        print("   7. Run diagnostic: python3 diagnose_camera.py")
        return False
    
    def get_frame(self):
        """‡∏£‡∏±‡∏ö‡πÄ‡∏ü‡∏£‡∏°‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏µ"""
        frame = None
        
        if self.camera_method == "picamera2":
            with self.buffer_lock:
                if self.frame_buffer is not None:
                    frame = self.frame_buffer.copy()
        elif self.cap and self.cap.isOpened():
            ret, frame = self.cap.read()
            if not ret:
                frame = None
        
        if frame is None:
            return None
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏™‡∏µ‡∏ï‡∏≤‡∏°‡πÇ‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
        if self.color_mode == "grayscale":
            if len(frame.shape) == 3:
                if self.camera_method == "picamera2":
                    # PiCamera2 RGB to Grayscale
                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
                else:
                    # OpenCV BGR to Grayscale
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                # ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô 3 channels ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
        
        elif self.color_mode == "color":
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô BGR ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OpenCV
            if self.camera_method == "picamera2" and len(frame.shape) == 3:
                # ‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å RGB ‡πÄ‡∏õ‡πá‡∏ô BGR
                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
        
        return frame
    
    def detect_emotion_deepface(self, frame):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏î‡πâ‡∏ß‡∏¢ DeepFace ‡πÅ‡∏ö‡∏ö real-time"""
        try:
            if not DEEPFACE_AVAILABLE:
                return self.detect_faces_simple(frame)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏Ñ‡∏ä
            frame_hash = hash(frame.tobytes())
            if frame_hash in self.emotion_cache:
                return self.emotion_cache[frame_hash]
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ó‡∏∏‡∏Å 50ms (20 FPS) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö real-time
            current_time = time.time()
            if current_time - self.last_emotion_time < DETECTION_INTERVAL:
                return None
            
            self.last_emotion_time = current_time
            
            if self.camera_method == "picamera2":
                if len(frame.shape) == 3 and frame.shape[2] == 3:
                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                else:
                    frame_bgr = frame
            else:
                frame_bgr = frame
                
            result = DeepFace.analyze(
                frame_bgr, 
                actions=['emotion'], 
                enforce_detection=False,
                silent=True
            )
            
            if isinstance(result, list) and len(result) > 0:
                emotion_map = {
                    'angry': 'Angry',
                    'disgust': 'Disgust',
                    'fear': 'Fear',
                    'happy': 'Happy',
                    'sad': 'Sad',
                    'surprise': 'Surprise',
                    'neutral': 'Neutral'
                }
                
                emotion = result[0]['dominant_emotion']
                confidence = result[0]['emotion'][emotion]
                
                # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≠‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≤‡∏Å DeepFace
                if 'region' in result[0]:
                    region = result[0]['region']
                    x, y, w, h = region['x'], region['y'], region['w'], region['h']
                    cv2.rectangle(frame_bgr, (x, y), (x+w, y+h), (0, 255, 0), 3)
                    cv2.putText(frame_bgr, f"AI: {emotion}", (x, y-15), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                # ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÉ‡∏´‡∏°‡πà
                assessed_emotion, satisfaction_text, satisfaction_level = self.get_emotion_assessment(confidence)
                
                result = (
                    assessed_emotion,
                    confidence,
                    satisfaction_level,
                    satisfaction_text
                )
                
                # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô‡πÅ‡∏Ñ‡∏ä
                self.emotion_cache[frame_hash] = result
                if len(self.emotion_cache) > EMOTION_CACHE_SIZE:
                    self.emotion_cache.pop(next(iter(self.emotion_cache)))
                
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏ó‡∏∏‡∏Å 5 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
                self.save_to_excel(*result)
                
                return result
            else:
                # ‡∏ñ‡πâ‡∏≤ DeepFace ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏´‡∏ô‡πâ‡∏≤ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ simple detection
                return self.detect_faces_simple(frame)
                
        except Exception as e:
            print(f"DeepFace error: {e}")
            return self.detect_faces_simple(frame)
    
    def get_current_emotion(self):
        """‡∏£‡∏±‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (AI ‡∏´‡∏£‡∏∑‡∏≠ Manual)"""
        current_time = time.time()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö manual emotion (‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ 5 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
        if (self.manual_emotion and 
            current_time - self.last_manual_time < 5.0):
            assessed_emotion, satisfaction_text, satisfaction_level = self.get_emotion_assessment(self.manual_confidence)
            return (
                f"[Manual] {assessed_emotion}",
                self.manual_confidence,
                satisfaction_level,
                satisfaction_text
            )
        
        return None
    
    def detect_faces_simple(self, frame):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏µ‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•"""
        try:
            if self.face_cascade is None:
                return "Face Detected", 50.0, 3, "***"
            
            # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏ü‡∏£‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
            enhanced_frame = self.enhance_frame_for_detection(frame)
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô grayscale ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö face detection
            if len(enhanced_frame.shape) == 3:
                gray = cv2.cvtColor(enhanced_frame, cv2.COLOR_BGR2GRAY)
            else:
                gray = enhanced_frame
            
            # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            faces = self.face_cascade.detectMultiScale(
                gray, 
                scaleFactor=1.05,  # ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
                minNeighbors=2,    # ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
                minSize=(30, 30),  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
                maxSize=(300, 300), # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏ô‡∏≤‡∏î‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
                flags=cv2.CASCADE_SCALE_IMAGE
            )
            
            # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≠‡∏ö‡∏£‡∏≠‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö
            if len(faces) > 0:
                # ‡∏´‡∏≤‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å)
                largest_face = max(faces, key=lambda face: face[2] * face[3])
                x, y, w, h = largest_face
                
                # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≠‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å (‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡πÄ‡∏Ç‡πâ‡∏°)
                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 3)
                
                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡πâ‡∏≤‡∏¢‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
                cv2.putText(frame, "FACE DETECTED", (x, y-15), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≠‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡πÜ (‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡∏≠‡πà‡∏≠‡∏ô)
                for (fx, fy, fw, fh) in faces:
                    if (fx, fy, fw, fh) != (x, y, w, h):  # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å
                        cv2.rectangle(frame, (fx, fy), (fx+fw, fy+fh), (0, 200, 0), 1)
                        cv2.putText(frame, "Face", (fx, fy-5), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 200, 0), 1)
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ
                cv2.putText(frame, f"Faces: {len(faces)}", (10, 30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                
                # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö
                assessed_emotion, satisfaction_text, satisfaction_level = self.get_emotion_assessment(65.0)
                result = (assessed_emotion, 65.0, satisfaction_level, satisfaction_text)
                
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏ó‡∏∏‡∏Å 5 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
                self.save_to_excel(*result)
                
                return result
            else:
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° "No Face Detected"
                height, width = frame.shape[:2]
                cv2.putText(frame, "NO FACE DETECTED", (width//2 - 150, height//2), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)
                cv2.putText(frame, "Please position your face in front of camera", 
                           (width//2 - 200, height//2 + 40), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 1)
                return "No Face", 0.0, 0, ""
                
        except Exception as e:
            print(f"Face detection error: {e}")
            return "Error", 0.0, 0, ""
    
    def add_overlay_info(self, frame, emotion, confidence, satisfaction_level, satisfaction_text):
        """‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡∏ö‡∏ô‡πÄ‡∏ü‡∏£‡∏° ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏µ"""
        if frame is None:
            return frame
        
        height, width = frame.shape[:2]
        
        # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÇ‡∏õ‡∏£‡πà‡∏á‡πÉ‡∏™ (‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏ï‡∏≤‡∏°‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠)
        overlay_height = max(200, int(height * 0.3))  # 30% ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏π‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (width-10, overlay_height), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # ‡∏Å‡∏£‡∏≠‡∏ö‡∏Ç‡∏≠‡∏ö
        cv2.rectangle(frame, (10, 10), (width-10, overlay_height), (255, 255, 255), 2)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏≤‡∏°‡∏Ç‡∏ô‡∏≤‡∏î‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠
        font_scale = max(0.8, min(2.0, width / 800))  # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠
        font_thickness = max(1, int(width / 400))  # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏ô‡∏≤‡∏ï‡∏≤‡∏°‡∏Ç‡∏ô‡∏≤‡∏î‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠
        
        # ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå (‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡∏™‡∏î‡πÉ‡∏™)
        emotion_text = f"Emotion: {emotion}"
        cv2.putText(frame, emotion_text, (20, 40), 
                   cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 255, 0), font_thickness)
        
        # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à (‡∏™‡∏µ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á)
        if isinstance(confidence, (int, float)) and confidence > 0:
            conf_text = f"Confidence: {confidence:.2f}%"
            cv2.putText(frame, conf_text, (20, 70), 
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale * 0.8, (0, 255, 255), font_thickness)
        
        # ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à (‡∏™‡∏µ‡∏ü‡πâ‡∏≤)
        satisfaction_level_text = f"Satisfaction: {satisfaction_level}/5"
        cv2.putText(frame, satisfaction_level_text, (20, 100), 
                   cv2.FONT_HERSHEY_SIMPLEX, font_scale * 0.8, (255, 255, 0), font_thickness)
        
        # ‡∏î‡∏≤‡∏ß‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à (‡∏™‡∏µ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á) - ‡πÉ‡∏ä‡πâ * ‡πÅ‡∏ó‡∏ô ‚òÖ
        stars_text = f"Score: {satisfaction_text}"
        cv2.putText(frame, stars_text, (20, 130), 
                   cv2.FONT_HERSHEY_SIMPLEX, font_scale * 0.8, (0, 255, 255), font_thickness)
        
        # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏•‡πâ‡∏≠‡∏á (‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß)
        camera_text = f"Camera: {self.camera_method}"
        cv2.putText(frame, camera_text, (20, 160), 
                   cv2.FONT_HERSHEY_SIMPLEX, font_scale * 0.6, (255, 255, 255), font_thickness)
        
        # ‡πÇ‡∏´‡∏°‡∏î‡∏™‡∏µ (‡∏™‡∏µ‡∏°‡πà‡∏ß‡∏á)
        color_text = f"Mode: {self.color_mode}"
        cv2.putText(frame, color_text, (20, 190), 
                   cv2.FONT_HERSHEY_SIMPLEX, font_scale * 0.6, (255, 0, 255), font_thickness)
        
        # ‡πÅ‡∏™‡∏î‡∏á manual controls
        manual_text = "Manual: 1=Happy 2=Neutral 3=Sad 4=Angry"
        cv2.putText(frame, manual_text, (20, 220), 
                   cv2.FONT_HERSHEY_SIMPLEX, font_scale * 0.5, (255, 255, 255), font_thickness)
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ manual mode
        if self.manual_emotion and time.time() - self.last_manual_time < 5.0:
            manual_status = f"Manual Active: {self.manual_emotion}"
            cv2.putText(frame, manual_status, (20, 250), 
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale * 0.6, (0, 255, 255), font_thickness)
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        current_time = time.time()
        time_since_save = current_time - self.last_data_save_time
        save_status = f"Next save in: {DATA_SAVE_INTERVAL - time_since_save:.1f}s"
        cv2.putText(frame, save_status, (20, 280), 
                   cv2.FONT_HERSHEY_SIMPLEX, font_scale * 0.5, (255, 255, 255), font_thickness)
        
        # ‡πÄ‡∏ß‡∏•‡∏≤ (‡∏™‡∏µ‡∏ü‡πâ‡∏≤) - ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡πÉ‡∏´‡∏ç‡πà
        time_text = datetime.now().strftime("%H:%M:%S")
        time_x = max(width - 200, width - int(width * 0.3))
        cv2.putText(frame, time_text, (time_x, 40), 
                   cv2.FONT_HERSHEY_SIMPLEX, font_scale * 0.8, (255, 255, 0), font_thickness)
        
        return frame
    
    def save_emotion_data(self, emotion, confidence):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå"""
        timestamp = datetime.now().isoformat()
        data = {
            'timestamp': timestamp,
            'emotion': emotion,
            'confidence': confidence,
            'camera_method': self.camera_method
        }
        self.emotion_history.append(data)
        
        if len(self.emotion_history) > 100:
            self.emotion_history.pop(0)
    
    def run(self):
        """‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
        print("üé≠ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏•‡πâ‡∏≠‡∏á")
        print("=" * 60)
        
        if not self.setup_camera():
            return
        
        print(f"üì∑ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {self.camera_method}")
        print("üéØ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡πÅ‡∏ö‡∏ö Real-time...")
        print("üíæ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏∏‡∏Å 5 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
        print(f"üìÅ Excel file: {os.path.abspath(self.excel_file)}")
        print("üìã ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°:")
        print("   - ‡∏Å‡∏î 'q' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏Å")
        print("   - ‡∏Å‡∏î 's' ‡∏´‡∏£‡∏∑‡∏≠ SPACE ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û")
        print("   - ‡∏Å‡∏î 'i' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏•‡πâ‡∏≠‡∏á")
        print("   - ‡∏Å‡∏î 'c' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏•‡∏±‡∏ö‡πÇ‡∏´‡∏°‡∏î‡∏™‡∏µ")
        print("   - ‡∏Å‡∏î 'f' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏•‡∏±‡∏ö‡πÇ‡∏´‡∏°‡∏î‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≠")
        print("   - ‡∏Å‡∏î 'b'/'v' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ß‡πà‡∏≤‡∏á (+/-)")
        print("   - ‡∏Å‡∏î 'n'/'m' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≠‡∏ô‡∏ó‡∏£‡∏≤‡∏™‡∏ï‡πå (+/-)")
        print("")
        print("üéØ Manual Emotion Input:")
        print("   - Press '1' = Happy (80-100%)")
        print("   - Press '2' = Neutral (50-70%)")
        print("   - Press '3' = Sad (30-40%)")
        print("   - Press '4' = Angry (0-20%)")
        print("=" * 60)
        
        self.is_running = True
        frame_count = 0
        fps_start_time = time.time()
        last_fps_update = time.time()
        fps = 0
        
        try:
            while True:
                frame = self.get_frame()
                
                if frame is None:
                    print("‚ùå Error: Can't receive frame")
                    time.sleep(0.1)
                    continue
                
                frame_count += 1
                
                # ‡πÑ‡∏°‡πà‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏ü‡∏£‡∏° (real-time detection)
                # if frame_count % FRAME_SKIP != 0:
                #     continue
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö manual emotion ‡∏Å‡πà‡∏≠‡∏ô
                manual_result = self.get_current_emotion()
                if manual_result:
                    emotion, confidence, satisfaction_level, satisfaction_text = manual_result
                else:
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏î‡πâ‡∏ß‡∏¢ AI
                    result = self.detect_emotion_deepface(frame)
                    if result:
                        emotion, confidence, satisfaction_level, satisfaction_text = result
                    else:
                        emotion, confidence, satisfaction_level, satisfaction_text = "No Face", 0.0, 0, ""
                
                display_frame = self.add_overlay_info(
                    frame, emotion, confidence, satisfaction_level, satisfaction_text
                )
                
                if display_frame is not None:
                    # ‡πÅ‡∏™‡∏î‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏Å‡∏ï‡∏¥
                    cv2.namedWindow('Emotion Detection', cv2.WINDOW_NORMAL)
                    if self.fullscreen_mode:
                        cv2.setWindowProperty('Emotion Detection', cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)
                    else:
                        cv2.setWindowProperty('Emotion Detection', cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_NORMAL)
                    cv2.imshow('Emotion Detection', display_frame)
                
                # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó FPS ‡∏ó‡∏∏‡∏Å 1 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
                current_time = time.time()
                if current_time - last_fps_update >= 1.0:
                    fps = frame_count / (current_time - fps_start_time)
                    frame_count = 0
                    fps_start_time = current_time
                    last_fps_update = current_time
                    print(f"üìä FPS: {fps:.1f}")
                
                # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ key input
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('s') or key == 32:
                    filename = f"emotion_screenshot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg"
                    cv2.imwrite(filename, display_frame)
                    print(f"üì∏ Screenshot saved: {filename}")
                elif key == ord('i'):
                    self.show_camera_info()
                elif key == ord('c'):
                    self.toggle_color_mode()
                elif key == ord('f'):
                    self.toggle_fullscreen()
                elif key == ord('b'):
                    self.adjust_brightness(0.1)
                elif key == ord('v'):
                    self.adjust_brightness(-0.1)
                elif key == ord('n'):
                    self.adjust_contrast(0.1)
                elif key == ord('m'):
                    self.adjust_contrast(-0.1)
                # Manual emotion input
                elif key == ord('1'):
                    self.set_manual_emotion("Happy", 90.0)
                elif key == ord('2'):
                    self.set_manual_emotion("Neutral", 60.0)
                elif key == ord('3'):
                    self.set_manual_emotion("Sad", 35.0)
                elif key == ord('4'):
                    self.set_manual_emotion("Angry", 10.0)
                
        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è Interrupted by user")
        
        finally:
            self.cleanup()
    
    def toggle_color_mode(self):
        """‡∏™‡∏•‡∏±‡∏ö‡πÇ‡∏´‡∏°‡∏î‡∏™‡∏µ"""
        if self.color_mode == "color":
            self.color_mode = "grayscale"
            print("üé® Switched to grayscale mode")
        else:
            self.color_mode = "color"
            print("üé® Switched to color mode")
    
    def toggle_fullscreen(self):
        """‡∏™‡∏•‡∏±‡∏ö‡πÇ‡∏´‡∏°‡∏î‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≠"""
        self.fullscreen_mode = not self.fullscreen_mode
        if self.fullscreen_mode:
            print("üñ•Ô∏è Switched to fullscreen mode")
        else:
            print("üñ•Ô∏è Switched to window mode")
    
    def adjust_brightness(self, delta):
        """‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ß‡πà‡∏≤‡∏á"""
        self.brightness = max(-1.0, min(1.0, self.brightness + delta))
        print(f"üí° Brightness: {self.brightness:.1f}")
        
        if self.picam2:
            try:
                self.picam2.set_controls({"Brightness": self.brightness})
            except:
                pass
    
    def adjust_contrast(self, delta):
        """‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≠‡∏ô‡∏ó‡∏£‡∏≤‡∏™‡∏ï‡πå"""
        self.contrast = max(0.0, min(2.0, self.contrast + delta))
        print(f"üîÜ Contrast: {self.contrast:.1f}")
        
        if self.picam2:
            try:
                self.picam2.set_controls({"Contrast": self.contrast})
            except:
                pass
    
    def show_camera_info(self):
        """‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏•‡πâ‡∏≠‡∏á"""
        print("\nüì∑ Camera Information:")
        print(f"   Method: {self.camera_method}")
        print(f"   Emotion history: {len(self.emotion_history)} records")
        print(f"   DeepFace available: {DEEPFACE_AVAILABLE}")
        print(f"   PiCamera2 available: {PICAMERA2_AVAILABLE}")
        
        if self.cap:
            width = self.cap.get(cv2.CAP_PROP_FRAME_WIDTH)
            height = self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
            fps = self.cap.get(cv2.CAP_PROP_FPS)
            print(f"   Resolution: {int(width)}x{int(height)}")
            print(f"   FPS Setting: {fps}")
    
    def cleanup(self):
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢"""
        print("üßπ Cleaning up...")
        
        self.is_running = False
        
        if self.picam2:
            try:
                self.picam2.stop()
            except:
                pass
        
        if self.cap:
            self.cap.release()
        
        cv2.destroyAllWindows()
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÉ‡∏ô‡∏Ñ‡∏¥‡∏ß
        try:
            while not self.data_queue.empty():
                data = self.data_queue.get_nowait()
                self._save_to_excel_internal(*data)
            self._save_excel_file()
        except:
            pass
        
        if self.emotion_history:
            print(f"üìä Total emotions detected: {len(self.emotion_history)}")
            
            emotions = [record['emotion'] for record in self.emotion_history]
            unique_emotions = set(emotions)
            print("üìà Emotion statistics:")
            for emotion in unique_emotions:
                count = emotions.count(emotion)
                percentage = (count / len(emotions)) * 100
                print(f"   {emotion}: {count} times ({percentage:.1f}%)")
        
        print("‚úÖ Cleanup completed")
    
    def test_face_detection(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤"""
        print("üß™ Testing face detection...")
        
        if not self.setup_camera():
            print("‚ùå Camera setup failed")
            return False
        
        print("üì∑ Camera ready, testing face detection...")
        print("Press 'q' to quit, 's' to save screenshot")
        
        frame_count = 0
        face_detected_count = 0
        
        try:
            while True:
                frame = self.get_frame()
                if frame is None:
                    continue
                
                frame_count += 1
                
                # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
                result = self.detect_faces_simple(frame)
                if result and result[0] != "No Face":
                    face_detected_count += 1
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
                cv2.imshow('Face Detection Test', frame)
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
                if frame_count % 30 == 0:  # ‡∏ó‡∏∏‡∏Å 30 ‡πÄ‡∏ü‡∏£‡∏°
                    detection_rate = (face_detected_count / frame_count) * 100
                    print(f"üìä Detection rate: {detection_rate:.1f}% ({face_detected_count}/{frame_count})")
                
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('s'):
                    filename = f"face_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg"
                    cv2.imwrite(filename, frame)
                    print(f"üì∏ Screenshot saved: {filename}")
        
        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è Test interrupted")
        
        finally:
            cv2.destroyAllWindows()
            if self.cap:
                self.cap.release()
            
            # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
            if frame_count > 0:
                detection_rate = (face_detected_count / frame_count) * 100
                print(f"\nüìä Final Detection Rate: {detection_rate:.1f}%")
                print(f"üìà Frames processed: {frame_count}")
                print(f"üë§ Faces detected: {face_detected_count}")
                
                if detection_rate > 80:
                    print("‚úÖ Face detection working well!")
                elif detection_rate > 50:
                    print("‚ö†Ô∏è Face detection working but could be better")
                else:
                    print("‚ùå Face detection needs improvement")
                    print("üí° Try:")
                    print("   - Better lighting")
                    print("   - Face directly in front of camera")
                    print("   - Remove glasses or hat")
                    print("   - Check camera focus")
            
            return detection_rate > 50 if frame_count > 0 else False

def main():
    print("üé≠ ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏•‡πâ‡∏≠‡∏á")
    print("üé® ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô 2.4 - Real-time Detection + Data Save Every 5s")
    print("=" * 70)
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
    print("\nüîß ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:")
    print("1. ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏õ‡∏Å‡∏ï‡∏¥")
    print("2. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤")
    
    while True:
        try:
            mode_choice = input("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î (1-2): ").strip()
            if mode_choice in ["1", "2"]:
                break
            else:
                print("‚ùå ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 1 ‡∏´‡∏£‡∏∑‡∏≠ 2")
        except:
            print("‚ùå ‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà")
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏•‡πâ‡∏≠‡∏á
    print("\nüîß ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏•‡πâ‡∏≠‡∏á:")
    print("1. ‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÄ‡∏ß‡πá‡∏ö‡πÅ‡∏Ñ‡∏° (‡πÇ‡∏ô‡πä‡∏ï‡∏ö‡∏∏‡πä‡∏Ñ)")
    print("2. ‡∏Å‡∏•‡πâ‡∏≠‡∏á Raspberry Pi")
    
    while True:
        try:
            choice = input("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏•‡πâ‡∏≠‡∏á (1-2): ").strip()
            if choice == "1":
                camera_type = "laptop"
                break
            elif choice == "2":
                camera_type = "pi"
                break
            else:
                print("‚ùå ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 1 ‡∏´‡∏£‡∏∑‡∏≠ 2")
        except:
            print("‚ùå ‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà")
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î‡∏™‡∏µ
    print("\nüîß ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•:")
    print("1. ‡πÇ‡∏´‡∏°‡∏î‡∏™‡∏µ (‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô)")
    print("2. ‡πÇ‡∏´‡∏°‡∏î‡∏Ç‡∏≤‡∏ß‡∏î‡∏≥")
    
    try:
        choice = input("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î (1-2, ‡∏Å‡∏î Enter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô): ").strip()
        color_mode = "grayscale" if choice == "2" else "color"
    except:
        color_mode = "color"
    
    detector = RaspberryPi4CameraDetector()
    detector.camera_type = camera_type
    detector.color_mode = color_mode
    detector.check_camera_hardware()
    
    if mode_choice == "1":
        detector.run()
    else:
        detector.test_face_detection()

if __name__ == "__main__":
    main()
    